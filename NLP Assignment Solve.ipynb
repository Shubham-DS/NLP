{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one python program in which you have to lowercase the sentence first \n",
    "# and than delete digits from the following sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "punct=string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in india people got affected with corona virus and are died\n"
     ]
    }
   ],
   "source": [
    "data=\"In India, 184 people got affected with Corona virus and 4 are died\"\n",
    "import nltk\n",
    "a=nltk.word_tokenize(data.lower())\n",
    "# print(a)\n",
    "clean=[]\n",
    "for i in a:\n",
    "    if i.isalpha():\n",
    "        clean.append(i)\n",
    "print(\" \".join(clean))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hope', 'built', 'savings', 'able', 'travel', 'hawai']\n"
     ]
    }
   ],
   "source": [
    "# Do stemming, lemmatization and tokenization from the following sentence.\n",
    "data1=\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\"\n",
    "import string\n",
    "punct=string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "clean_data =[]\n",
    "for i in nltk.word_tokenize(data1.lower()):\n",
    "    if i not in stop_words:\n",
    "        if i not in punct:\n",
    "            clean_data.append(i)\n",
    "print(clean_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope hop hope built built built save sav save abl abl abl travel travel travel hawai hawa hawai "
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "for i in clean_data:\n",
    "    from nltk.stem import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    print(porter.stem(i),end=\" \")\n",
    "    from nltk.stem import LancasterStemmer\n",
    "    lancaster = LancasterStemmer()\n",
    "    print(lancaster.stem(i),end=\" \") \n",
    "    from nltk.stem import SnowballStemmer\n",
    "    SnowBall=SnowballStemmer(\"english\")\n",
    "    print(SnowBall.stem(i),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope built saving able travel hawai "
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "for i in clean_data:\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemma=WordNetLemmatizer()\n",
    "    print(lemma.lemmatize(i),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'l', 'N', 'n', 'y']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one python program from the following sentence.\n",
    "# \"I love NLP, not you\"\n",
    "# output : ['I', 'l', 'N', 'n', 'y']\n",
    "str=\"I love NLP, not you\"\n",
    "[i[0] for i in str.split()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "\n",
    "# \"xyz@gmail.com\",\n",
    "# \"abc@yahoo.com\",\n",
    "# \"xyz@hotmail.com\",\n",
    "# \"abc@ineuron.ai\",\n",
    "# \"xyz@outlook.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xyz', 'gmail.com\",\"abc', 'yahoo.com\",\"xyz', 'hotmail.com\",\"abc', 'ineuron.ai\",\"xyz', 'outlook.com']\n"
     ]
    }
   ],
   "source": [
    "str=\"\"\"xyz@gmail.com\",\"abc@yahoo.com\",\"xyz@hotmail.com\",\"abc@ineuron.ai\",\"xyz@outlook.com\"\"\"\n",
    "import re\n",
    "res=re.split(\"@\",str)\n",
    "print(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "# [\"New Delhi is the capital of India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delhi is the capital of India'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str=[\"New Delhi is the capital of India\"]\n",
    "string=\" \".join(my_str)\n",
    "# print(string)\n",
    "import re\n",
    "op = re.sub(r'\\bNew\\b\\s+',\"\",string) \n",
    "op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textblob - Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Susie works in a shoeshine shop. Where she shines she sits, and where she sits she shines\")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "A = TextBlob(\"Susie works in a shoeshine shop. Where she shines she sits, and where she sits she shines\")\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Susie', 'NNP'), ('works', 'VBZ'), ('in', 'IN'), ('a', 'DT'), ('shoeshine', 'NN'), ('shop', 'NN'), ('Where', 'WRB'), ('she', 'PRP'), ('shines', 'VBZ'), ('she', 'PRP'), ('sits', 'VBZ'), ('and', 'CC'), ('where', 'WRB'), ('she', 'PRP'), ('sits', 'VBZ'), ('she', 'PRP'), ('shines', 'NNS')] "
     ]
    }
   ],
   "source": [
    "print(A.tags,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['susie', 'shoeshine shop'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"डेटा एक नया तेल है।\")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate the following sentences in your own language using the textblob.\n",
    "my_str1=\"Data is a new oil.\"\n",
    "# \"A.I is the last invention\",\n",
    "# \"She sells seashells by the seashore\",\n",
    "# \"He threw three free throws\".\n",
    "from textblob import TextBlob\n",
    "new=TextBlob(\"Data is a new oil.\")\n",
    "new.translate(to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"वह समुंद्र के किनारे शँख बेचती है\")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=TextBlob(\"She sells seashells by the seashore\")\n",
    "new.translate(to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"A.I अंतिम आविष्कार है\")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=TextBlob(\"A.I is the last invention\")\n",
    "new.translate(to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"उसने तीन मुफ्त फेंके\")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=TextBlob(\"He threw three free throws\")\n",
    "new.translate(to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe=TextBlob(\"How much wood would a woodchuck chuck if a woodchuck could chuck wood?He would chuck, he would, as much as he could, and chuck as much wood As a woodchuck would if a woodchuck could chuck wood\")\n",
    "qwe.words.count('wood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the string: \n",
      "Giutar is a beutiful insturmant\n",
      "Guitar is a beautiful instrument\n"
     ]
    }
   ],
   "source": [
    "# Create a spell checker program using the textblob library with using your own sentences.\n",
    "from textblob import TextBlob\n",
    "string=input(\"Enter the string: \\n\")\n",
    "new1=TextBlob(string)\n",
    "new1=new1.correct()\n",
    "print(new1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
